https://lingma.aliyun.com
Qwen3-Coder-480B-A35B-Instruct
480B 完整尺寸的模型参数大小
A35B，激活的参数量是35B

https://modelscope.cn/search?search=deepseek-r1
7B => GPU
1.5B => CPU 内存

https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B

# 模型下载
from modelscope import snapshot_download
snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', cache_dir="/root/autodl-tmp/models")

deepseek-ai/DeepSeek-R1-Distill-Qwen-7B => Model ID

方法1：vllm进行部署
方法2：python 调用部署好的模型
方法3：ollama

{'response': '<think>\n\n</think>\n\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。我是一个AI assistant，通过机器学习技术来模拟人类思考和判断的能力。我理解用户的问题会通过这些正式的、诚实且专业的语言形式呈现，我会以友好、理性的态度为您提供详细的回答。'}

封装成API接口，给其他人用
方法1：fastapi
方法2：flask

约束1：模型尺寸，不会太大
约束2：上网环境

感觉对于我们学员来说，私有化部署是个熟悉使用模型的过程，有助于以后实战微调

deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B 与 Ollama运行deepseek-r1:1.5b是一样的吗？
是一样的参数

自己在魔搭平台下的大模型怎么跑？只能用ollama吗
方法1：ollama
方法2：vllm 
方法3：python调用大模型

服务器上怎么部署 调用
Step1，通过modelscope下载对应的大模型
# 模型下载
from modelscope import snapshot_download
snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', cache_dir="/root/autodl-tmp/models")

Step2，deepseek-r1-7b使用

请从伊利集团财务报表中提取以下信息，包括：公司名称，股票代码，营收，净利润，毛利，总资产，总负债。并以JSON格式返回。

你是一个数学助手，请根据以下步骤计算用户输入的金额。请将每个金额首先加上1000元，接着减去500元，然后乘以1.2输出计算结果，以','作为分隔符进行返回。
你可以参考以下计算过程来帮助解决：
"""
对于输入：2000, 3000, 4000
计算过程如下：
首先分别对输入的2000, 3000, 4000加上1000，得到：3000, 4000, 5000
然后将3000, 4000, 5000分别减去500，得到：2500, 3500, 4500
然后将2500, 3500, 4500分别乘以1.2，得到：3000, 4200, 5400
答案是：3000, 4200, 5400
"""
输入：1500, 2500, 3500


你是一个客户服务助手，请按照以下步骤处理客户的投诉。首先，记录客户的投诉类型，然后确定处理优先级（高、中、低），接着分配给适当的部门，最后生成一个处理跟踪编号并直接输出。
计算过程示例：
"""
客户投诉：客服的态度冷淡，没有耐心解答我的问题。
处理过程如下：
首先记录客户的投诉类型，得到：客服态度差
然后确定处理优先级，得到：优先级高
然后分配给适当的部门，得到：客服部
最后生成一个跟踪编号，得到 KF10001
"""
客户投诉：我的账户意外冻结了。我并未收到任何提前通知或解释。

4090 * 4
Q：在企业实际工作开发中，私有化大模型是使用服务器部署为主还是本地部署为主
服务器

Q：私有化部署需要GPU，那么api部署需要GPU吗？
需要，API只是接口封装

Q：fastapi封装的前提是，先用ollama部署本地模型吗，还是直接调用大模型的Open-API
可以先用ollama本地部署后，会有一个11434的端口

Q：fastapi是用ollama pull安装吗

Q：promt 给出后，如果得不到想要的结果，对于 promt的调试，有没有什么方式或者方法？ promt 可以让大模型学习， 它和训练是否等同?
可以抛给大模型
你的回答，我感觉不是想要的结果。
你觉得关于这个问题，你还有哪些需要了解的

Q：本地部署了小版本的大模型之后，还能继续训练大模型学一些专业知识库吗？
不冲突
部署了小尺寸大模型，当然可以针对这个小尺寸大模型进行微调 => 新参数  => 部署和使用

不难,但没想明白底层关系:用户、AI应用开发、大模型三者中,prompt是由谁编写
.如由用户编写,为什么会出现prompt工程师这个岗位,这个岗位做什么事情
====
Prompt = 人写的
用户的提问，可以是LLM中内置的人设

每个人都可以给LLM下指令，让LLM成为助手 => 提示词工程师角色
程序化
system_prompt: 扮演一个给公司起名字的一个助手
user_prompt: 彩色花朵


老师，我的问题是vLLM是否安全，因为是米国伯克利的产品。是否满足信息安全审计要求，国内是否有平替产品，我相信国内很多企业会有这个问题。
moonshot 开发了底层的部署

提示词工程师职责：1.写role为system的提示词；2.封装应用，根据应用场景特性，制定user的提示词模板，让用户写提示词更简单

