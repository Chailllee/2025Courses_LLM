{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cea150f",
   "metadata": {},
   "source": [
    "# preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9a60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dashscope\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8db01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get API key\n",
    "# dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "api_key = os.environ.get('DASHSCOPE_API_KEY')\n",
    "dashscope.api_key = 'sk-16f2c80d9d3e4d7d9cc2f6b43f22dfbd' # https://bailian.console.aliyun.com/?tab=model#/api-key\n",
    "# dashscope.api_key ='sk-qzkddkwofxndanqzonvabhwqbpwganpxkuuoghtxgvmzygnt' # https://cloud.siliconflow.cn/me/account/ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c771a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q addict\n",
    "# %pip install -q \"modelscope==1.25.0\"\n",
    "# %pip install -U \"modelscope[aigc]\"\n",
    "# from modelscope import AutoModel, AutoTokenizer\n",
    "# from modelscope import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa0408",
   "metadata": {},
   "source": [
    "# Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b308f",
   "metadata": {},
   "source": [
    "## 2-2 Mobile phone service subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use deepseek-v3 model\n",
    "def get_completion(prompt, model=\"deepseek-v3\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = dashscope.Generation.call(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        result_format=\"message\",\n",
    "        temperature=0, # 模型输出的随机性，0 表示随机性最小\n",
    "    )\n",
    "    return response.output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e79e2",
   "metadata": {},
   "source": [
    "### 使用提示词完成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt\n",
    "instructions = \"\"\"\n",
    "你的任务是识别用户对手机流量套餐产品的选择条件。\n",
    "每种流量套餐产品包含三个属性：名称，月费价格，月流量。\n",
    "根据用户输入，识别用户在上述三种属性上的需求是什么。\n",
    "\"\"\"\n",
    "\n",
    "# user prompt\n",
    "input_text = \"\"\"办个100G的套餐。\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{instructions}\n",
    "{input_text}\n",
    "\"\"\"\n",
    "\n",
    "print(\"==== Prompt ====\")\n",
    "print(prompt)\n",
    "print(\"================\")\n",
    "\n",
    "# use model to get response\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76ded2",
   "metadata": {},
   "source": [
    "### JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfcd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_format = \"\"\"以 JSON 格式输出\"\"\"\n",
    "\n",
    "prompt=f\"\"\"\n",
    "{instructions}\n",
    "{output_format}\n",
    "{input_text}\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eab37a",
   "metadata": {},
   "source": [
    "### CoT case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "给定一段用户与手机流量套餐客服的对话，。\n",
    "你的任务是判断客服的回答是否符合下面的规范：\n",
    "\n",
    "- 必须有礼貌\n",
    "- 必须用官方口吻，不能使用网络用语\n",
    "- 介绍套餐时，必须准确提及产品名称、月费价格和月流量总量。上述信息缺失一项或多项，或信息与事实不符，都算信息不准确\n",
    "- 不可以是话题终结者\n",
    "\n",
    "已知产品包括：\n",
    "\n",
    "经济套餐：月费50元，月流量10G\n",
    "畅游套餐：月费180元，月流量100G\n",
    "无限套餐：月费300元，月流量1000G\n",
    "校园套餐：月费150元，月流量200G，限在校学生办理\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output_format = \"\"\"\n",
    "如果符合规范，输出：Y\n",
    "如果不符合规范，输出：N\n",
    "\"\"\"\n",
    "\n",
    "context = \"\"\"\n",
    "用户：你们有什么流量大的套餐\n",
    "客服：亲，我们现在正在推广无限套餐，每月300元就可以享受1000G流量，您感兴趣吗？\n",
    "\"\"\"\n",
    "\n",
    "# cot=\"\"\n",
    "cot=\"请一步一步分析对话\"\n",
    "\n",
    "prompt=f\"\"\"\n",
    "{instructions}\n",
    "{cot}\n",
    "{output_format}\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "response=get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51453d",
   "metadata": {},
   "source": [
    "### 使用Prompt调优Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89089613",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "做一个手机流量套餐的客服代表，叫小瓜。可以帮助用户选择最合适的流量套餐产品。可以选择的套餐包括：\n",
    "经济套餐，月费50元，10G流量；\n",
    "畅游套餐，月费180元，100G流量；\n",
    "无限套餐，月费300元，1000G流量；\n",
    "校园套餐，月费150元，200G流量，仅限在校生。\"\"\"\n",
    "\n",
    "instruction = \"\"\"\n",
    "你是一名专业的提示词创作者。你的目标是帮助我根据需求打造更好的提示词。\n",
    "\n",
    "你将生成以下部分：\n",
    "提示词：{根据我的需求提供更好的提示词}\n",
    "优化建议：{用简练段落分析如何改进提示词，需给出严格批判性建议}\n",
    "问题示例：{提出最多3个问题，以用于和用户更好的交流}\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{instruction}\n",
    "{user_prompt}\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775aa7b",
   "metadata": {},
   "source": [
    "## 2-3 DeepSeek-r1-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069e47a",
   "metadata": {},
   "source": [
    "https://lingma.aliyun.com\n",
    "Qwen3-Coder-480B-A35B-Instruct\n",
    "480B 完整尺寸的模型参数大小\n",
    "A35B，激活的参数量是35B\n",
    "\n",
    "https://modelscope.cn/search?search=deepseek-r1\n",
    "7B => GPU\n",
    "1.5B => CPU 内存\n",
    "\n",
    "https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
    "\n",
    "**模型下载**\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', cache_dir=\"/root/autodl-tmp/models\")\n",
    "\n",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B => Model ID\n",
    "\n",
    "方法1：vllm进行部署\n",
    "方法2：python 调用部署好的模型\n",
    "方法3：ollama\n",
    "\n",
    "{'response': '<think>\\n\\n</think>\\n\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。我是一个AI assistant，通过机器学习技术来模拟人类思考和判断的能力。我理解用户的问题会通过这些正式的、诚实且专业的语言形式呈现，我会以友好、理性的态度为您提供详细的回答。'}\n",
    "\n",
    "封装成API接口，给其他人用\n",
    "方法1：fastapi\n",
    "方法2：flask\n",
    "\n",
    "约束1：模型尺寸，不会太大\n",
    "约束2：上网环境\n",
    "\n",
    "感觉对于我们学员来说，私有化部署是个熟悉使用模型的过程，有助于以后实战微调\n",
    "\n",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B 与 Ollama运行deepseek-r1:1.5b是一样的吗？\n",
    "是一样的参数\n",
    "\n",
    "自己在魔搭平台下的大模型怎么跑？只能用ollama吗\n",
    "方法1：ollama\n",
    "方法2：vllm \n",
    "方法3：python调用大模型\n",
    "\n",
    "服务器上怎么部署 调用\n",
    "Step1，通过modelscope下载对应的大模型\n",
    "\n",
    "**模型下载**\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', cache_dir=\"/root/autodl-tmp/models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ad77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q modelscope\n",
    "# from modelscope import snapshot_download\n",
    "\n",
    "# model_dir = snapshot_download(\n",
    "# \t'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n",
    "# \tcache_dir=r\"d:\\Job_hunt\\2025Courses_LLM\\models\\deepseek\"\n",
    "# )\n",
    "# print(f\"Model downloaded to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a854527",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# delete model\n",
    "import shutil\n",
    "# shutil.rmtree(\"/root/autodl-tmp/models\") \n",
    "shutil.rmtree(r\"D:\\Job_hunt\\2025Courses_LLM\\models\\deepseek\\deepseek-ai\")\n",
    "# d:\\Job_hunt\\2025Courses_LLM\\models\\deepseek\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39279d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q addict\n",
    "# %pip install -q \"modelscope==1.25.0\"\n",
    "# %pip install -U \"modelscope[aigc]\"\n",
    "# from modelscope import AutoModel, AutoTokenizer\n",
    "# from modelscope import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad5895",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457861a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "BASE = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama():\n",
    "    try:\n",
    "        r = requests.get(f\"{BASE}/api/tags\", timeout=5)\n",
    "        r.raise_for_status()\n",
    "        tags = r.json().get(\"models\", [])\n",
    "        print(\"✅ Ollama reachable. Installed models:\")\n",
    "        for m in tags:\n",
    "            print(\" -\", m.get(\"name\"))\n",
    "        if not tags:\n",
    "            print(\"(No models listed. 如果已下载但未显示，请在终端运行: ollama pull deepseek-r1:1.5b)\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Ollama not reachable:\", e)\n",
    "        print(\"请在终端运行: ollama serve\")\n",
    "        return\n",
    "    \n",
    "    # 简单生成接口测试（若模型已安装）\n",
    "    test_model = \"deepseek-r1:1.5b\"\n",
    "    try:\n",
    "        gen = requests.post(f\"{BASE}/api/generate\", json={\n",
    "            \"model\": test_model,\n",
    "            \"prompt\": \"你好，做一个简短的自我介绍。\",\n",
    "            \"stream\": False\n",
    "        }, timeout=30)\n",
    "        if gen.status_code == 200:\n",
    "            payload = gen.json()\n",
    "            print(\"✅ Generate ok | snippet:\", (payload.get(\"response\") or \"\")[:120])\n",
    "        else:\n",
    "            print(\"⚠️ Generate status:\", gen.status_code)\n",
    "            print(\"Body:\", gen.text[:200])\n",
    "            print(\"如果是模型未安装，请在终端运行: ollama pull\", test_model)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Generate failed:\", e)\n",
    "\n",
    "check_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b62ca",
   "metadata": {},
   "source": [
    "### Step 0: Ollama 健康检查（本机）\n",
    "先检查本机是否运行了 Ollama 服务（默认 http://localhost:11434），并验证模型标签是否可用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf3b2b",
   "metadata": {},
   "source": [
    "### Ollama REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c585471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(prompt, model=\"deepseek-r1:1.5b\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False  # 设置为 True 可以获取流式响应\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"response\"]\n",
    "    else:\n",
    "        raise Exception(f\"API 请求失败: {response.text}\")\n",
    "\n",
    "# 使用示例\n",
    "response = query_ollama(\"你好，请介绍一下你自己\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24cfd93",
   "metadata": {},
   "source": [
    "### Ollama stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65334385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama_stream(prompt, model=\"deepseek-r1:1.5b\",stream=False):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\":stream # 设置为 True 可以获取流式响应\n",
    "    }\n",
    "\n",
    "    if stream:\n",
    "        # 流式相应处理\n",
    "        with requests.post(url, json=data, stream=True) as response:\n",
    "            if response.status_code == 200:\n",
    "                #  逐行打印流式响应内容\n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    if line:\n",
    "                        # Ollama流式返回每行是一个json字符串\n",
    "                        try:\n",
    "                            # import json\n",
    "                            obj = json.loads(line)\n",
    "                            print(obj.get(\"response\",\"\"), end=\"\", flush=True)\n",
    "                        except Exception as e:\n",
    "                            print(f\"解析流式响应出错: {e}\")\n",
    "            else:\n",
    "                raise Exception(f\"API 请求失败: {response.text}\")\n",
    "            \n",
    "    else:\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            raise Exception(f\"API 请求失败: {response.text}\")\n",
    "        \n",
    "# print(\"\\n=== 流式响应 ===\")\n",
    "# query_ollama_stream(\"帮我写一个二分查找法\",stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab559d",
   "metadata": {},
   "source": [
    "### Ollama FastAPI封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfba5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q fastapi uvicorn\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc6f391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FastAPI app 已定义（带健壮错误处理）\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Step 1: 定义 FastAPI 应用 =====\n",
    "from fastapi.responses import JSONResponse\n",
    "app = FastAPI()\n",
    "\n",
    "# 定义请求模型\n",
    "class ChatRequest(BaseModel):\n",
    "    prompt: str\n",
    "    model: str = \"deepseek-r1:1.5b\"\n",
    "\n",
    "# 允许跨域请求（根据需要配置）\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/api/chat\")\n",
    "async def chat(request: ChatRequest):\n",
    "    ollama_url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": request.model,\n",
    "        \"prompt\": request.prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(ollama_url, json=data, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        ctype = (response.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        body_text = response.text\n",
    "\n",
    "        if not body_text.strip():\n",
    "            return JSONResponse(status_code=502, content={\n",
    "                \"error\": \"Upstream returned empty body\"\n",
    "            })\n",
    "        elif \"application/json\" in ctype or body_text.lstrip().startswith((\"{\", \"[\")):\n",
    "            try:\n",
    "                payload = response.json()\n",
    "            except (json.JSONDecodeError, ValueError):\n",
    "                return JSONResponse(status_code=502, content={\n",
    "                    \"error\": \"Upstream returned invalid JSON\",\n",
    "                    \"body_snippet\": body_text[:200]\n",
    "                })\n",
    "            value = payload.get(\"response\")\n",
    "            if value is None:\n",
    "                return JSONResponse(status_code=502, content={\n",
    "                    \"error\": \"Upstream JSON missing 'response'\",\n",
    "                    \"payload_keys\": list(payload.keys())\n",
    "                })\n",
    "            return {\"response\": value}\n",
    "        else:\n",
    "            return JSONResponse(status_code=502, content={\n",
    "                \"error\": \"Upstream returned non-JSON\",\n",
    "                \"content_type\": ctype,\n",
    "                \"body_snippet\": body_text[:200]\n",
    "            })\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        return JSONResponse(status_code=503, content={\n",
    "            \"error\": \"Cannot reach Ollama at localhost:11434\",\n",
    "            \"details\": str(e)\n",
    "        })\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        return JSONResponse(status_code=504, content={\n",
    "            \"error\": \"Upstream timeout\",\n",
    "            \"details\": str(e)\n",
    "        })\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return JSONResponse(status_code=response.status_code, content={\n",
    "            \"error\": \"Upstream HTTP error\",\n",
    "            \"details\": (response.text or str(e))[:200]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return JSONResponse(status_code=500, content={\n",
    "            \"error\": \"Server internal error\",\n",
    "            \"details\": str(e)[:200]\n",
    "        })\n",
    "\n",
    "print(\"✅ FastAPI app 已定义（带健壮错误处理）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9566fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FastAPI server 已在后台启动 (http://localhost:8000)\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 2: 在后台启动服务器 =====\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def start_server():\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# 等待服务器启动\n",
    "time.sleep(3)\n",
    "print(\"✅ FastAPI server 已在后台启动 (http://localhost:8000)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f78e20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': '您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。我擅长通过思考来帮您解答复杂的数学，代码和逻辑推理等理工类问题。'}\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 3: 客户端调用 =====\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 多等一会儿确保服务器已就绪\n",
    "time.sleep(1)\n",
    "\n",
    "url = \"http://localhost:8000/api/chat\"\n",
    "payload = {\n",
    "    \"model\": \"deepseek-r1:1.5b\",\n",
    "    \"prompt\": \"你好，请介绍一下你自己\"\n",
    "}\n",
    "\n",
    "# 简单重试/退避，缓解服务刚启动或网络瞬断\n",
    "max_attempts = 3\n",
    "backoff_seconds = [0, 1, 2]\n",
    "last_exc = None\n",
    "response = None\n",
    "for attempt in range(max_attempts):\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=30)\n",
    "        break\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        last_exc = e\n",
    "        if attempt < max_attempts - 1:\n",
    "            time.sleep(backoff_seconds[attempt])\n",
    "        else:\n",
    "            print(f\"Request failed after {max_attempts} attempts: {e}\")\n",
    "\n",
    "if response is None:\n",
    "    # 已在循环内打印失败信息\n",
    "    pass\n",
    "else:\n",
    "    # 避免 JSONDecodeError 的健壮输出处理\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "        ctype = (response.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        body = response.text\n",
    "\n",
    "        if not body.strip():\n",
    "            print(\"Empty response body\")\n",
    "        elif \"application/json\" in ctype or body.lstrip().startswith((\"{\", \"[\")):\n",
    "            print(response.json())\n",
    "        else:\n",
    "            print(\"Non-JSON response (showing first 200 chars):\", repr(body[:200]))\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error {response.status_code}: {e}\")\n",
    "    except (json.JSONDecodeError, ValueError):\n",
    "        print(\"Invalid JSON (first 200 chars):\", repr(response.text[:200]))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
